{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkApp-DF-Imp-Functions\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkApp-DF-Imp-Functions</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x180a8ed7220>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Creating a dummy dataframe\n",
    "df = spark.range(1)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date and Time Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### current_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|current_date()|\n",
      "+--------------+\n",
      "|    2024-12-02|\n",
      "+--------------+\n",
      "\n",
      "root\n",
      " |-- current_date(): date (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "df.select(current_date()).show()\n",
    "df.select(current_date()).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### to_date()\n",
    "Converts the string type date in to date type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 2024-12-02: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.select(lit(\"2024-12-02\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|formatted_date|\n",
      "+--------------+\n",
      "|    2024-12-02|\n",
      "+--------------+\n",
      "\n",
      "root\n",
      " |-- formatted_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, lit\n",
    "\n",
    "format_date_df = df.select(to_date(col=lit(\"2024-12-02\"), format=\"yyyy-MM-dd\").alias(\"formatted_date\"))\n",
    "format_date_df.show()\n",
    "format_date_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### date_format()\n",
    "Returns the new date format (in string type) as specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|formatted_date|\n",
      "+--------------+\n",
      "|    12/02/2024|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format, lit\n",
    "\n",
    "df.select(date_format(date=lit(\"2024-12-02\"), format=\"MM/dd/yyyy\").alias(\"formatted_date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|new_format|\n",
      "+----------+\n",
      "|12/02/2024|\n",
      "+----------+\n",
      "\n",
      "root\n",
      " |-- new_format: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = format_date_df.select(date_format(\"formatted_date\", \"MM/dd/yyyy\").alias(\"new_format\"))\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|        d1|        d2|\n",
      "+----------+----------+\n",
      "|2024-12-01|2024-12-14|\n",
      "|2024-12-01|2024-07-17|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"2024-12-01\", \"2024-12-14\"), (\"2024-12-01\", \"2024-07-17\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=[\"d1\", \"d2\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+\n",
      "|        d1|        d2|datediff|\n",
      "+----------+----------+--------+\n",
      "|2024-12-01|2024-12-14|      13|\n",
      "|2024-12-01|2024-07-17|    -137|\n",
      "+----------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff\n",
    "\n",
    "df.select(\"d1\", \"d2\", datediff(start=\"d1\", end=\"d2\").alias(\"datediff\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|        d2|add_d2_days|\n",
      "+----------+-----------+\n",
      "|2024-12-14| 2024-12-24|\n",
      "|2024-07-17| 2024-07-27|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add\n",
    "\n",
    "df.select(\"d2\", date_add(start=\"d2\", days=10).alias(\"add_d2_days\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|        d2|d2_date_sub|\n",
      "+----------+-----------+\n",
      "|2024-12-14| 2024-12-04|\n",
      "|2024-07-17| 2024-07-07|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_sub\n",
    "\n",
    "df.select(\"d2\", date_sub(start=\"d2\", days=10).alias(\"d2_date_sub\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------+\n",
      "|        d1|        d2|months_between|\n",
      "+----------+----------+--------------+\n",
      "|2024-12-01|2024-12-14|   -0.41935484|\n",
      "|2024-12-01|2024-07-17|    4.48387097|\n",
      "+----------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import months_between\n",
    "\n",
    "df.select(\"d1\", \"d2\", months_between(date1=\"d1\", date2=\"d2\").alias(\"months_between\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|        d2|add_d2_months|\n",
      "+----------+-------------+\n",
      "|2024-12-14|   2025-05-14|\n",
      "|2024-07-17|   2024-12-17|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import add_months\n",
    "\n",
    "df.select(\"d2\", add_months(start=\"d2\", months=5).alias(\"add_d2_months\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|current_timestamp()       |\n",
      "+--------------------------+\n",
      "|2024-12-02 11:28:03.794376|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "df.select(current_timestamp()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 2024-12-02 11:28:03: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select(lit(\"2024-12-02 11:28:03\"))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|timestamp          |\n",
      "+-------------------+\n",
      "|2024-12-02 11:28:03|\n",
      "+-------------------+\n",
      "\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "df = df.select(to_timestamp(lit(\"2024-12-02 11:28:03\")).alias(\"timestamp\"))\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|year(timestamp)|\n",
      "+---------------+\n",
      "|2024           |\n",
      "+---------------+\n",
      "\n",
      "+----------------+\n",
      "|month(timestamp)|\n",
      "+----------------+\n",
      "|12              |\n",
      "+----------------+\n",
      "\n",
      "+--------------+\n",
      "|day(timestamp)|\n",
      "+--------------+\n",
      "|2             |\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import day, year, month\n",
    "\n",
    "df.select(year(\"timestamp\")).show(truncate=False)\n",
    "df.select(month(\"timestamp\")).show(truncate=False)\n",
    "df.select(day(\"timestamp\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|second(timestamp)|\n",
      "+-----------------+\n",
      "|3                |\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import second\n",
    "\n",
    "df.select(second(\"timestamp\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Functions\n",
    "- count, count_distinct\n",
    "- sum, avg\n",
    "- min, max\n",
    "- collect_list and collect_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------+------+\n",
      "| ID|          Name| Department|Salary|\n",
      "+---+--------------+-----------+------+\n",
      "|  1|      John Doe|Engineering| 50000|\n",
      "|  2|    Jane Smith|  Marketing| 45000|\n",
      "|  3|     Jim Brown|      Sales| 40000|\n",
      "|  4|  Jackie White|         HR| 42000|\n",
      "|  5|   Emily Davis|Engineering| 60000|\n",
      "|  6| Michael Scott| Management| 75000|\n",
      "|  7|    Pam Beesly|  Reception| 35000|\n",
      "|  8|Dwight Schrute|      Sales| 50000|\n",
      "|  9| Angela Martin| Accounting| 48000|\n",
      "| 10|  Kevin Malone| Accounting| 45000|\n",
      "| 11|Oscar Martinez| Accounting| 47000|\n",
      "| 12|Stanley Hudson|      Sales| 46000|\n",
      "+---+--------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"resources/in/employee/employee_data_1.csv\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|Total_No_Of_Employees|\n",
      "+---------------------+\n",
      "|                   12|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "df.select(count(col=\"ID\").alias(\"Total_No_Of_Employees\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|Total_Departments|\n",
      "+-----------------+\n",
      "|                7|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count_distinct\n",
    "\n",
    "df.select(count_distinct(col=\"Department\").alias(\"Total_Departments\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|    Average_Salary|\n",
      "+------------------+\n",
      "|48583.333333333336|\n",
      "+------------------+\n",
      "\n",
      "+--------------+\n",
      "|Minimum_Salary|\n",
      "+--------------+\n",
      "|         35000|\n",
      "+--------------+\n",
      "\n",
      "+--------------+\n",
      "|Maximum_Salary|\n",
      "+--------------+\n",
      "|         75000|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, min, max\n",
    "\n",
    "df.select(avg(col=\"Salary\").alias(\"Average_Salary\")).show()\n",
    "df.select(min(col=\"Salary\").alias(\"Minimum_Salary\")).show()\n",
    "df.select(max(col=\"Salary\").alias(\"Maximum_Salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|Total_Budget|\n",
      "+------------+\n",
      "|      583000|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df.select(sum(col=\"Salary\").alias(\"Total_Budget\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collect_list()\n",
    "Return all values of a column including duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|All_Employees                                                                                                                                                       |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[John Doe, Jane Smith, Jim Brown, Jackie White, Emily Davis, Michael Scott, Pam Beesly, Dwight Schrute, Angela Martin, Kevin Malone, Oscar Martinez, Stanley Hudson]|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "df.select(collect_list(col=\"Name\").alias(\"All_Employees\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collect_set()\n",
    "Return only distinct values of a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+\n",
      "|Departments                                                           |\n",
      "+----------------------------------------------------------------------+\n",
      "|[Accounting, Management, Reception, HR, Sales, Marketing, Engineering]|\n",
      "+----------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set\n",
    "\n",
    "df.select(collect_set(col=\"Department\").alias(\"Departments\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Functions\n",
    "\n",
    "Window functions are applied on window/subset of records. Data is partioned and ordered by the column used for patitioning.\n",
    "\n",
    "- rank - Assigns rank value from 1. Skips the rank if there are ties or same value.\n",
    "- dense_rank - Assigns rank value from 1. Does not skip the rank even if there are ties or same value.\n",
    "- row_number: Assigns values sequentially from 1 to n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+----------+\n",
      "| id|   name|salary|department|\n",
      "+---+-------+------+----------+\n",
      "|  1|   Paul| 32000|        IT|\n",
      "|  2| Angela| 45000|        HR|\n",
      "|  3|   John| 38000|        IT|\n",
      "|  4|Micheal| 36000|        HR|\n",
      "|  5|  Oscar| 34000|        HR|\n",
      "|  6|  Emily| 32000|        IT|\n",
      "|  7|    Pam| 24000| Reception|\n",
      "|  8| Dwight| 42000|        IT|\n",
      "+---+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, \"Paul\", 32000, \"IT\"), \n",
    "    (2, \"Angela\", 45000, \"HR\"), \n",
    "    (3, \"John\", 38000, \"IT\"), \n",
    "    (4, \"Micheal\", 36000, \"HR\"), \n",
    "    (5, \"Oscar\", 34000, \"HR\"), \n",
    "    (6, \"Emily\", 32000, \"IT\"),\n",
    "    (7, \"Pam\", 24000, \"Reception\"),\n",
    "    (8, \"Dwight\", 42000, \"IT\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=[\"id\", \"name\", \"salary\", \"department\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = Window.partitionBy(\"department\").orderBy(\"salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+----------+----------+\n",
      "| id|   name|salary|department|row_number|\n",
      "+---+-------+------+----------+----------+\n",
      "|  5|  Oscar| 34000|        HR|         1|\n",
      "|  4|Micheal| 36000|        HR|         2|\n",
      "|  2| Angela| 45000|        HR|         3|\n",
      "|  1|   Paul| 32000|        IT|         1|\n",
      "|  6|  Emily| 32000|        IT|         2|\n",
      "|  3|   John| 38000|        IT|         3|\n",
      "|  8| Dwight| 42000|        IT|         4|\n",
      "|  7|    Pam| 24000| Reception|         1|\n",
      "+---+-------+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "df.withColumn(\"row_number\", row_number().over(window)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+----------+----+\n",
      "| id|   name|salary|department|rank|\n",
      "+---+-------+------+----------+----+\n",
      "|  5|  Oscar| 34000|        HR|   1|\n",
      "|  4|Micheal| 36000|        HR|   2|\n",
      "|  2| Angela| 45000|        HR|   3|\n",
      "|  1|   Paul| 32000|        IT|   1|\n",
      "|  6|  Emily| 32000|        IT|   1|\n",
      "|  3|   John| 38000|        IT|   3|\n",
      "|  8| Dwight| 42000|        IT|   4|\n",
      "|  7|    Pam| 24000| Reception|   1|\n",
      "+---+-------+------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "\n",
    "df.withColumn(\"rank\", rank().over(window)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+----------+----+\n",
      "| id|   name|salary|department|rank|\n",
      "+---+-------+------+----------+----+\n",
      "|  5|  Oscar| 34000|        HR|   1|\n",
      "|  4|Micheal| 36000|        HR|   2|\n",
      "|  2| Angela| 45000|        HR|   3|\n",
      "|  1|   Paul| 32000|        IT|   1|\n",
      "|  6|  Emily| 32000|        IT|   1|\n",
      "|  3|   John| 38000|        IT|   2|\n",
      "|  8| Dwight| 42000|        IT|   3|\n",
      "|  7|    Pam| 24000| Reception|   1|\n",
      "+---+-------+------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank\n",
    "\n",
    "df.withColumn(\"rank\", dense_rank().over(window)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
