{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkApp-DF-Tranformations\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkApp-DF-Tranformations</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1b99f61d900>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"resources/in/employee/employee_data_1.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------+\n",
      "| ID|        Name| Department|Salary|\n",
      "+---+------------+-----------+------+\n",
      "|  1|    John Doe|Engineering| 50000|\n",
      "|  2|  Jane Smith|  Marketing| 45000|\n",
      "|  3|   Jim Brown|      Sales| 40000|\n",
      "|  4|Jackie White|         HR| 42000|\n",
      "|  5| Emily Davis|Engineering| 60000|\n",
      "+---+------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| ID|      Name| Department|Salary|\n",
      "+---+----------+-----------+------+\n",
      "|  1|  John Doe|Engineering| 50000|\n",
      "|  2|Jane Smith|  Marketing| 45000|\n",
      "|  3| Jim Brown|      Sales| 40000|\n",
      "+---+----------+-----------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"*\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      Name|\n",
      "+----------+\n",
      "|  John Doe|\n",
      "|Jane Smith|\n",
      "| Jim Brown|\n",
      "+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.Name).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|      Name| Department|\n",
      "+----------+-----------+\n",
      "|  John Doe|Engineering|\n",
      "|Jane Smith|  Marketing|\n",
      "| Jim Brown|      Sales|\n",
      "+----------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.select(df.Name, df[\"Department\"]).show(3)\n",
    "df.select([df.Name, df[\"Department\"]]).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### withColumn()\n",
    "\n",
    "Apply transformations to a column of dataframe. It return new dataframe.\n",
    "\n",
    "1. Convert the datatypes of columns\n",
    "2. Create new columns or replace the existing columns\n",
    "3. Transform entire columns with values\n",
    "4. Concate the columns etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(colName=\"Salary\", col=df.Salary.cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+-------+\n",
      "| ID|        Name| Department| Salary|\n",
      "+---+------------+-----------+-------+\n",
      "|  1|    John Doe|Engineering|50000.0|\n",
      "|  2|  Jane Smith|  Marketing|45000.0|\n",
      "|  3|   Jim Brown|      Sales|40000.0|\n",
      "|  4|Jackie White|         HR|42000.0|\n",
      "|  5| Emily Davis|Engineering|60000.0|\n",
      "+---+------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(colName=\"Bonus_Salary\", col=df.Salary * 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+-------+------------+\n",
      "| ID|        Name| Department| Salary|Bonus_Salary|\n",
      "+---+------------+-----------+-------+------------+\n",
      "|  1|    John Doe|Engineering|50000.0|     12500.0|\n",
      "|  2|  Jane Smith|  Marketing|45000.0|     11250.0|\n",
      "|  3|   Jim Brown|      Sales|40000.0|     10000.0|\n",
      "|  4|Jackie White|         HR|42000.0|     10500.0|\n",
      "|  5| Emily Davis|Engineering|60000.0|     15000.0|\n",
      "+---+------------+-----------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(colName=\"Total_Salary\", col=df.Salary + df.Bonus_Salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+-------+------------+------------+\n",
      "| ID|        Name| Department| Salary|Bonus_Salary|Total_Salary|\n",
      "+---+------------+-----------+-------+------------+------------+\n",
      "|  1|    John Doe|Engineering|50000.0|     12500.0|     62500.0|\n",
      "|  2|  Jane Smith|  Marketing|45000.0|     11250.0|     56250.0|\n",
      "|  3|   Jim Brown|      Sales|40000.0|     10000.0|     50000.0|\n",
      "|  4|Jackie White|         HR|42000.0|     10500.0|     52500.0|\n",
      "|  5| Emily Davis|Engineering|60000.0|     15000.0|     75000.0|\n",
      "+---+------------+-----------+-------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### withColumnRenamed()\n",
    "\n",
    "Rename the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(existing=\"Name\", new=\"Emp_Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+-------+------------+------------+\n",
      "| ID|  Emp_Name| Department| Salary|Bonus_Salary|Total_Salary|\n",
      "+---+----------+-----------+-------+------------+------------+\n",
      "|  1|  John Doe|Engineering|50000.0|     12500.0|     62500.0|\n",
      "|  2|Jane Smith|  Marketing|45000.0|     11250.0|     56250.0|\n",
      "|  3| Jim Brown|      Sales|40000.0|     10000.0|     50000.0|\n",
      "+---+----------+-----------+-------+------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop()\n",
    "Delete or remove the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"Bonus_Salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+-------+------------+\n",
      "| ID|  Emp_Name| Department| Salary|Total_Salary|\n",
      "+---+----------+-----------+-------+------------+\n",
      "|  1|  John Doe|Engineering|50000.0|     62500.0|\n",
      "|  2|Jane Smith|  Marketing|45000.0|     56250.0|\n",
      "|  3| Jim Brown|      Sales|40000.0|     50000.0|\n",
      "+---+----------+-----------+-------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array Functions\n",
    "1. explode()\n",
    "2. split()\n",
    "3. array()\n",
    "4. array_contains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(100, [\"PC\", \"Monitor\", \"Keyboard\"]), (101, [\"Laptop\", \"Speaker\"]), (102, [\"Mouse\", \"Adapter\"]), (103, [\"Headphone\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = false)\n",
      " |-- items: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(name=\"order_id\", dataType=IntegerType(), nullable=False),\n",
    "    StructField(name=\"items\", dataType=ArrayType(elementType=StringType()), nullable=False)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------+\n",
      "|order_id|items                  |\n",
      "+--------+-----------------------+\n",
      "|100     |[PC, Monitor, Keyboard]|\n",
      "|101     |[Laptop, Speaker]      |\n",
      "|102     |[Mouse, Adapter]       |\n",
      "|103     |[Headphone]            |\n",
      "+--------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|order_id|     item|\n",
      "+--------+---------+\n",
      "|     100|       PC|\n",
      "|     100|  Monitor|\n",
      "|     100| Keyboard|\n",
      "|     101|   Laptop|\n",
      "|     101|  Speaker|\n",
      "|     102|    Mouse|\n",
      "|     102|  Adapter|\n",
      "|     103|Headphone|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df = df.withColumn(\"item\", explode(df.items))\n",
    "df = df.drop(\"items\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- items: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(100, \"PC,Monitor,Keyboard\"), (101, \"Laptop,Speaker\"), (102, \"Mouse,Adapter\"), (103, \"Headphone\")]\n",
    "df = spark.createDataFrame(data=data, schema=[\"order_id\", \"items\"])\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|order_id|              items|\n",
      "+--------+-------------------+\n",
      "|     100|PC,Monitor,Keyboard|\n",
      "|     101|     Laptop,Speaker|\n",
      "|     102|      Mouse,Adapter|\n",
      "|     103|          Headphone|\n",
      "+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "df = df.withColumn(\"items\", split(df.items, \",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------+\n",
      "|order_id|items                  |\n",
      "+--------+-----------------------+\n",
      "|100     |[PC, Monitor, Keyboard]|\n",
      "|101     |[Laptop, Speaker]      |\n",
      "|102     |[Mouse, Adapter]       |\n",
      "|103     |[Headphone]            |\n",
      "+--------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- country_codes: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(100, [\"US\", \"UK\", \"AUS\"]), (101, [\"US\", \"UK\"]), (102, [\"IND\", \"UK\"]), (103, [\"IND\"])]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=[\"id\", \"country_codes\"])\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-------+\n",
      "| id|country_codes|isLocal|\n",
      "+---+-------------+-------+\n",
      "|100|[US, UK, AUS]|   true|\n",
      "|101|     [US, UK]|   true|\n",
      "|102|    [IND, UK]|  false|\n",
      "|103|        [IND]|  false|\n",
      "+---+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains, col\n",
    "\n",
    "df = df.withColumn(\"isLocal\", array_contains(col(\"country_codes\"), \"US\"))\n",
    "#df = df.withColumn(\"isLocal\", array_contains(df.country_codes, \"US\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+\n",
      "| id|first_name|last_name|\n",
      "+---+----------+---------+\n",
      "|100|      Paul|  Brandon|\n",
      "|101|      John|      Doe|\n",
      "|102|      Tina|   Nailor|\n",
      "+---+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(100, \"Paul\", \"Brandon\"), (101, \"John\", \"Doe\"), (102, \"Tina\", \"Nailor\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=[\"id\", \"first_name\", \"last_name\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+---------------+\n",
      "| id|first_name|last_name|      full_name|\n",
      "+---+----------+---------+---------------+\n",
      "|100|      Paul|  Brandon|[Paul, Brandon]|\n",
      "|101|      John|      Doe|    [John, Doe]|\n",
      "|102|      Tina|   Nailor| [Tina, Nailor]|\n",
      "+---+----------+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array\n",
    "\n",
    "df = df.withColumn(\"full_name\", array(df.first_name, df.last_name))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Functions\n",
    "1. explode()\n",
    "2. map_keys()\n",
    "3. map_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------------------+\n",
      "|user_id|users_info                      |\n",
      "+-------+--------------------------------+\n",
      "|1      |{gender -> male, name -> Paul}  |\n",
      "|2      |{gender -> female, name -> Tina}|\n",
      "|3      |{gender -> male, name -> John}  |\n",
      "+-------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, {\"name\": \"Paul\", \"gender\": \"male\"}), (2, {\"name\": \"Tina\", \"gender\": \"female\"}), (3, {\"name\": \"John\", \"gender\": \"male\"})]\n",
    "\n",
    "from pyspark.sql.types import StringType, IntegerType, MapType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(name=\"user_id\", dataType=IntegerType()),\n",
    "    StructField(name=\"users_info\", dataType=MapType(keyType=StringType(), valueType=StringType()))\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------------------+-----------+\n",
      "|user_id|users_info                      |user_gender|\n",
      "+-------+--------------------------------+-----------+\n",
      "|1      |{gender -> male, name -> Paul}  |male       |\n",
      "|2      |{gender -> female, name -> Tina}|female     |\n",
      "|3      |{gender -> male, name -> John}  |male       |\n",
      "+-------+--------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"user_gender\", df.users_info.gender)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|user_id|user_gender|\n",
      "+-------+-----------+\n",
      "|      1|       male|\n",
      "|      2|     female|\n",
      "|      3|       male|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"user_id\", \"user_gender\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|user_id|   key| value|\n",
      "+-------+------+------+\n",
      "|      1|gender|  male|\n",
      "|      1|  name|  Paul|\n",
      "|      2|gender|female|\n",
      "|      2|  name|  Tina|\n",
      "|      3|gender|  male|\n",
      "|      3|  name|  John|\n",
      "+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df.select(\"user_id\", explode(df.users_info)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------+\n",
      "|user_id|          users_info|user_gender|\n",
      "+-------+--------------------+-----------+\n",
      "|      1|{gender -> male, ...|       male|\n",
      "|      2|{gender -> female...|     female|\n",
      "|      3|{gender -> male, ...|       male|\n",
      "+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------+--------------+\n",
      "|user_id|          users_info|user_gender|     info_keys|\n",
      "+-------+--------------------+-----------+--------------+\n",
      "|      1|{gender -> male, ...|       male|[gender, name]|\n",
      "|      2|{gender -> female...|     female|[gender, name]|\n",
      "|      3|{gender -> male, ...|       male|[gender, name]|\n",
      "+-------+--------------------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_keys\n",
    "\n",
    "df.withColumn(\"info_keys\", map_keys(df.users_info)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------+--------------+\n",
      "|user_id|          users_info|user_gender|   info_values|\n",
      "+-------+--------------------+-----------+--------------+\n",
      "|      1|{gender -> male, ...|       male|  [male, Paul]|\n",
      "|      2|{gender -> female...|     female|[female, Tina]|\n",
      "|      3|{gender -> male, ...|       male|  [male, John]|\n",
      "+-------+--------------------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_values\n",
    "\n",
    "df.withColumn(\"info_values\", map_values(df.users_info)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### when() and otherwise()\n",
    "Similar to if and else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "| id|name|age|\n",
      "+---+----+---+\n",
      "|  1|Paul| 32|\n",
      "|  2|Tina| 45|\n",
      "|  3|John| 28|\n",
      "|  4|Mike| 36|\n",
      "+---+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"Paul\", 32), (2, \"Tina\", 45), (3, \"John\", 28), (4, \"Mike\", 36)]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=[\"id\", \"name\", \"age\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+\n",
      "| id|name| position|\n",
      "+---+----+---------+\n",
      "|  1|Paul|   Senior|\n",
      "|  2|Tina|     Lead|\n",
      "|  3|John|Associate|\n",
      "|  4|Mike|   Senior|\n",
      "+---+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df.select(df.id, df.name, when(condition=df.age >= 40, value=\"Lead\").when(condition=(df.age >=30) & (df.age < 40), value=\"Senior\").otherwise(value=\"Associate\").alias(\"position\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### alias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----------+\n",
      "|emp_id|emp_name|age_in_yrs|\n",
      "+------+--------+----------+\n",
      "|     1|    Paul|        32|\n",
      "|     2|    Tina|        45|\n",
      "|     3|    John|        28|\n",
      "|     4|    Mike|        36|\n",
      "+------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.id.alias(\"emp_id\"), df.name.alias(\"emp_name\"), df.age.alias(\"age_in_yrs\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df_new = df.select(df.id.cast(\"int\"), df.name, df.age.cast(IntegerType()))\n",
    "df_new.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter() or where()\n",
    "Both are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+----------+\n",
      "| id| name|age|department|\n",
      "+---+-----+---+----------+\n",
      "|  1| Paul| 32|        HR|\n",
      "|  2| Tina| 45|        HR|\n",
      "|  3| John| 28|        IT|\n",
      "|  4| Mike| 36|        IT|\n",
      "|  5|David| 34|     Sales|\n",
      "+---+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"Paul\", 32, \"HR\"), (2, \"Tina\", 45, \"HR\"), (3, \"John\", 28, \"IT\"), (4, \"Mike\", 36, \"IT\"), (5, \"David\", 34, \"Sales\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=[\"id\", \"name\", \"age\", \"department\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+----------+\n",
      "| id| name|age|department|\n",
      "+---+-----+---+----------+\n",
      "|  1| Paul| 32|        HR|\n",
      "|  2| Tina| 45|        HR|\n",
      "|  3| John| 28|        IT|\n",
      "|  4| Mike| 36|        IT|\n",
      "|  5|David| 34|     Sales|\n",
      "+---+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+----------+\n",
      "| id|name|age|department|\n",
      "+---+----+---+----------+\n",
      "|  2|Tina| 45|        HR|\n",
      "|  4|Mike| 36|        IT|\n",
      "+---+----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.id, df.name, df.age, df.department).filter(df.age >= 35).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "| id|name|age|\n",
      "+---+----+---+\n",
      "|  3|John| 28|\n",
      "|  4|Mike| 36|\n",
      "+---+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.id, df.name, df.age).filter(df.department == \"IT\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "| id|name|age|\n",
      "+---+----+---+\n",
      "|  2|Tina| 45|\n",
      "+---+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.id, df.name, df.age).filter((df.age >= 35) & (df.department == \"HR\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### like()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|name|\n",
      "+---+----+\n",
      "|  2|Tina|\n",
      "|  4|Mike|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.id, df.name).filter(df.name.like(\"_i%\")).show()  #second letter is i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distinct()\n",
    "\n",
    "Returns only distinct or unique rows considering all columns values. Removes the duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+----------+\n",
      "| id| name|age|department|\n",
      "+---+-----+---+----------+\n",
      "|  1| Paul| 32|        HR|\n",
      "|  2| Tina| 45|        HR|\n",
      "|  3| John| 28|        IT|\n",
      "|  4| Mike| 36|        IT|\n",
      "|  5|David| 34|     Sales|\n",
      "|  5|David| 34|     Sales|\n",
      "|  2| Tina| 45|        HR|\n",
      "+---+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"Paul\", 32, \"HR\"), (2, \"Tina\", 45, \"HR\"), (3, \"John\", 28, \"IT\"), (4, \"Mike\", 36, \"IT\"), (5, \"David\", 34, \"Sales\"), (5, \"David\", 34, \"Sales\"), (2, \"Tina\", 45, \"HR\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=[\"id\", \"name\", \"age\", \"department\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+----------+\n",
      "| id| name|age|department|\n",
      "+---+-----+---+----------+\n",
      "|  1| Paul| 32|        HR|\n",
      "|  2| Tina| 45|        HR|\n",
      "|  3| John| 28|        IT|\n",
      "|  4| Mike| 36|        IT|\n",
      "|  5|David| 34|     Sales|\n",
      "+---+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop_duplicates()\n",
    "\n",
    "Returns only distinct or unique rows based on subset of columns selected. Removes the duplicate rows.\n",
    "By default, all columns will be considered if no columns passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+----------+\n",
      "| id| name|age|department|\n",
      "+---+-----+---+----------+\n",
      "|  1| Paul| 32|        HR|\n",
      "|  2| Tina| 45|        HR|\n",
      "|  3| John| 28|        IT|\n",
      "|  4| Mike| 36|        IT|\n",
      "|  5|David| 34|     Sales|\n",
      "+---+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+----------+\n",
      "| id| name|age|department|\n",
      "+---+-----+---+----------+\n",
      "|  1| Paul| 32|        HR|\n",
      "|  3| John| 28|        IT|\n",
      "|  5|David| 34|     Sales|\n",
      "+---+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates([\"department\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sort() or orderBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+----------+\n",
      "| id| name|age|department|\n",
      "+---+-----+---+----------+\n",
      "|  1| Paul| 32|        HR|\n",
      "|  2| Tina| 45|        HR|\n",
      "|  3| John| 28|        IT|\n",
      "|  4| Mike| 36|        IT|\n",
      "|  5|David| 34|     Sales|\n",
      "+---+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"Paul\", 32, \"HR\"), (2, \"Tina\", 45, \"HR\"), (3, \"John\", 28, \"IT\"), (4, \"Mike\", 36, \"IT\"), (5, \"David\", 34, \"Sales\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=[\"id\", \"name\", \"age\", \"department\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### asc() and desc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+----------+\n",
      "| id| name|age|department|\n",
      "+---+-----+---+----------+\n",
      "|  3| John| 28|        IT|\n",
      "|  1| Paul| 32|        HR|\n",
      "|  5|David| 34|     Sales|\n",
      "|  4| Mike| 36|        IT|\n",
      "|  2| Tina| 45|        HR|\n",
      "+---+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.sort(df.age).show() #default is ascending order\n",
    "df.sort(df.age.asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+----------+\n",
      "| id| name|age|department|\n",
      "+---+-----+---+----------+\n",
      "|  2| Tina| 45|        HR|\n",
      "|  4| Mike| 36|        IT|\n",
      "|  5|David| 34|     Sales|\n",
      "|  1| Paul| 32|        HR|\n",
      "|  3| John| 28|        IT|\n",
      "+---+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.age.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+----------+\n",
      "| id| name|age|department|\n",
      "+---+-----+---+----------+\n",
      "|  5|David| 34|     Sales|\n",
      "|  4| Mike| 36|        IT|\n",
      "|  3| John| 28|        IT|\n",
      "|  2| Tina| 45|        HR|\n",
      "|  1| Paul| 32|        HR|\n",
      "+---+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.department.desc(), df.age.desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### union() and unionAll()\n",
    "1. Combine rows of two dataframes.\n",
    "2. But do not remove duplicates.\n",
    "3. Both union() and unionAll() have same functionality\n",
    "4. Schema of both dataframes must be same (as union is performed based on column positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [(1, \"Paul\", 32, \"HR\"), (2, \"Tina\", 45, \"HR\"), (3, \"John\", 28, \"IT\")]\n",
    "data2 = [(3, \"John\", 28, \"IT\"), (4, \"Mike\", 36, \"IT\"), (5, \"David\", 34, \"Sales\")]\n",
    "\n",
    "schema = [\"id\", \"name\", \"age\", \"department\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data=data1, schema=schema)\n",
    "df2 = spark.createDataFrame(data=data2, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1:\n",
      "+---+----+---+----------+\n",
      "| id|name|age|department|\n",
      "+---+----+---+----------+\n",
      "|  1|Paul| 32|        HR|\n",
      "|  2|Tina| 45|        HR|\n",
      "|  3|John| 28|        IT|\n",
      "+---+----+---+----------+\n",
      "\n",
      "df2:\n",
      "+---+-----+---+----------+\n",
      "| id| name|age|department|\n",
      "+---+-----+---+----------+\n",
      "|  3| John| 28|        IT|\n",
      "|  4| Mike| 36|        IT|\n",
      "|  5|David| 34|     Sales|\n",
      "+---+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"df1:\")\n",
    "df1.show()\n",
    "\n",
    "print(\"df2:\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+----------+\n",
      "| id| name|age|department|\n",
      "+---+-----+---+----------+\n",
      "|  1| Paul| 32|        HR|\n",
      "|  2| Tina| 45|        HR|\n",
      "|  3| John| 28|        IT|\n",
      "|  3| John| 28|        IT|\n",
      "|  4| Mike| 36|        IT|\n",
      "|  5|David| 34|     Sales|\n",
      "+---+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.union(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+----------+\n",
      "| id| name|age|department|\n",
      "+---+-----+---+----------+\n",
      "|  1| Paul| 32|        HR|\n",
      "|  2| Tina| 45|        HR|\n",
      "|  3| John| 28|        IT|\n",
      "|  3| John| 28|        IT|\n",
      "|  4| Mike| 36|        IT|\n",
      "|  5|David| 34|     Sales|\n",
      "+---+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.unionAll(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+----------+\n",
      "| id| name|age|department|\n",
      "+---+-----+---+----------+\n",
      "|  1| Paul| 32|        HR|\n",
      "|  2| Tina| 45|        HR|\n",
      "|  3| John| 28|        IT|\n",
      "|  4| Mike| 36|        IT|\n",
      "|  5|David| 34|     Sales|\n",
      "+---+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.union(df2).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unionByName()\n",
    "\n",
    "1. Combine rows of two dataframes even with different schema.\n",
    "2. Need to pass allowMissingColumns=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1:\n",
      "+---+----+---+\n",
      "| id|name|age|\n",
      "+---+----+---+\n",
      "|  1|Paul| 32|\n",
      "|  2|Tina| 45|\n",
      "|  3|John| 28|\n",
      "+---+----+---+\n",
      "\n",
      "df2:\n",
      "+---+-----+----------+\n",
      "| id| name|department|\n",
      "+---+-----+----------+\n",
      "|  3| John|        IT|\n",
      "|  4| Mike|        HR|\n",
      "|  5|David|     Sales|\n",
      "+---+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1 = [(1, \"Paul\", 32), (2, \"Tina\", 45), (3, \"John\", 28)]\n",
    "data2 = [(3, \"John\", \"IT\"), (4, \"Mike\", \"HR\"), (5, \"David\", \"Sales\")]\n",
    "\n",
    "schema1 = [\"id\", \"name\", \"age\"]\n",
    "schema2 = [\"id\", \"name\", \"department\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data=data1, schema=schema1)\n",
    "df2 = spark.createDataFrame(data=data2, schema=schema2)\n",
    "\n",
    "print(\"df1:\")\n",
    "df1.show()\n",
    "\n",
    "print(\"df2:\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+\n",
      "| id| name|  age|\n",
      "+---+-----+-----+\n",
      "|  1| Paul|   32|\n",
      "|  2| Tina|   45|\n",
      "|  3| John|   28|\n",
      "|  3| John|   IT|\n",
      "|  4| Mike|   HR|\n",
      "|  5|David|Sales|\n",
      "+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.union(df2).show()\n",
    "# This is wrong actually because union merges based on column position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+----------+\n",
      "| id| name| age|department|\n",
      "+---+-----+----+----------+\n",
      "|  1| Paul|  32|      NULL|\n",
      "|  2| Tina|  45|      NULL|\n",
      "|  3| John|  28|      NULL|\n",
      "|  3| John|NULL|        IT|\n",
      "|  4| Mike|NULL|        HR|\n",
      "|  5|David|NULL|     Sales|\n",
      "+---+-----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.unionByName(df2, allowMissingColumns=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupBy() and agg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+---------+--------------------+-----+\n",
      "|      Category|Price|ProductID|         ProductName|Stock|\n",
      "+--------------+-----+---------+--------------------+-----+\n",
      "|    Smartphone|  999|     P001|     Apple iPhone 15|   50|\n",
      "|    Smartphone|  899|     P002|  Samsung Galaxy S23|   30|\n",
      "|    Headphones|  299|     P003|     Sony WH-1000XM5|  100|\n",
      "|        Laptop| 1199|     P004|         Dell XPS 13|   20|\n",
      "|        Laptop| 1399|     P005|     HP Spectre x360|   15|\n",
      "|    Headphones|  329|     P006|Bose QuietComfort 45|   80|\n",
      "|      Wearable|  499|     P007|Apple Watch Series 9|   60|\n",
      "|      Wearable|  399|     P008|Samsung Galaxy Wa...|   70|\n",
      "|      E-Reader|  129|     P009|   Kindle Paperwhite|  120|\n",
      "|   Accessories|   99|     P010|Logitech MX Master 3|   90|\n",
      "|   Accessories|   69|     P011| Razer DeathAdder V2|  100|\n",
      "|    Smartphone|  799|     P012|      Google Pixel 8|   40|\n",
      "|        Laptop| 1499|     P013|      Lenovo Yoga 9i|   25|\n",
      "|        Camera| 1999|     P014|         Sony A7 III|   10|\n",
      "|        Camera| 2499|     P015|        Canon EOS R6|    8|\n",
      "|         Drone|  799|     P016|     DJI Mavic Air 2|   15|\n",
      "|        Tablet|  999|     P017|Microsoft Surface...|   18|\n",
      "|        Tablet|  849|     P018|Samsung Galaxy Ta...|   20|\n",
      "|Gaming Console|  499|     P019|  Sony PlayStation 5|   50|\n",
      "|Gaming Console|  499|     P020|       Xbox Series X|   45|\n",
      "+--------------+-----+---------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(path=\"resources/in/product_inventory/product_inventory_multiline.json\", multiLine=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|      Category|count|\n",
      "+--------------+-----+\n",
      "|      E-Reader|    1|\n",
      "|      Wearable|    2|\n",
      "|        Laptop|    3|\n",
      "|   Accessories|    2|\n",
      "|        Camera|    2|\n",
      "|        Tablet|    2|\n",
      "|         Drone|    1|\n",
      "|    Smartphone|    3|\n",
      "|    Headphones|    2|\n",
      "|Gaming Console|    2|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Category\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+\n",
      "|      Category|count(Category)|\n",
      "+--------------+---------------+\n",
      "|      E-Reader|              1|\n",
      "|      Wearable|              2|\n",
      "|        Laptop|              3|\n",
      "|   Accessories|              2|\n",
      "|        Camera|              2|\n",
      "|        Tablet|              2|\n",
      "|         Drone|              1|\n",
      "|    Smartphone|              3|\n",
      "|    Headphones|              2|\n",
      "|Gaming Console|              2|\n",
      "+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Category\").agg({\"Category\": \"count\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|      Category|sum(price)|\n",
      "+--------------+----------+\n",
      "|      E-Reader|       129|\n",
      "|      Wearable|       898|\n",
      "|        Laptop|      4097|\n",
      "|   Accessories|       168|\n",
      "|        Camera|      4498|\n",
      "|        Tablet|      1848|\n",
      "|         Drone|       799|\n",
      "|    Smartphone|      2697|\n",
      "|    Headphones|       628|\n",
      "|Gaming Console|       998|\n",
      "+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Category\").agg({\"price\": \"sum\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|      Category|total_price|\n",
      "+--------------+-----------+\n",
      "|      E-Reader|        129|\n",
      "|      Wearable|        898|\n",
      "|        Laptop|       4097|\n",
      "|   Accessories|        168|\n",
      "|        Camera|       4498|\n",
      "|        Tablet|       1848|\n",
      "|         Drone|        799|\n",
      "|    Smartphone|       2697|\n",
      "|    Headphones|        628|\n",
      "|Gaming Console|        998|\n",
      "+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "df.groupBy(\"Category\").agg(sum(\"price\").alias(\"total_price\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|      Category|max(stock)|\n",
      "+--------------+----------+\n",
      "|      E-Reader|       120|\n",
      "|      Wearable|        70|\n",
      "|        Laptop|        25|\n",
      "|   Accessories|       100|\n",
      "|        Camera|        10|\n",
      "|        Tablet|        20|\n",
      "|         Drone|        15|\n",
      "|    Smartphone|        50|\n",
      "|    Headphones|       100|\n",
      "|Gaming Console|        50|\n",
      "+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Category\").agg({\"stock\": \"max\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+---------+---------+------------------+\n",
      "|      Category|total_count|min_stock|max_stock|         avg_price|\n",
      "+--------------+-----------+---------+---------+------------------+\n",
      "|      E-Reader|          1|      120|      120|             129.0|\n",
      "|      Wearable|          2|       60|       70|             449.0|\n",
      "|        Laptop|          3|       15|       25|1365.6666666666667|\n",
      "|   Accessories|          2|       90|      100|              84.0|\n",
      "|        Camera|          2|        8|       10|            2249.0|\n",
      "|        Tablet|          2|       18|       20|             924.0|\n",
      "|         Drone|          1|       15|       15|             799.0|\n",
      "|    Smartphone|          3|       30|       50|             899.0|\n",
      "|    Headphones|          2|       80|      100|             314.0|\n",
      "|Gaming Console|          2|       45|       50|             499.0|\n",
      "+--------------+-----------+---------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, min, max, avg\n",
    "\n",
    "df.groupBy(\"Category\").agg(count(\"Category\").alias(\"total_count\"), min(\"stock\").alias(\"min_stock\"), max(\"stock\").alias(\"max_stock\"), avg(\"price\").alias(\"avg_price\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### join()\n",
    "1. inner join\n",
    "2. left join\n",
    "3. rigth join\n",
    "4. full join\n",
    "5. self join\n",
    "6. leftsemi and leftanti joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-------+\n",
      "| id|emp_name|age|dept_id|\n",
      "+---+--------+---+-------+\n",
      "|101|    Paul| 32|      2|\n",
      "|102|    Tina| 45|      3|\n",
      "|103|    John| 28|      2|\n",
      "|104|    Mike| 36|      1|\n",
      "|105|   David| 34|      2|\n",
      "|106|  Wright| 25|      5|\n",
      "+---+--------+---+-------+\n",
      "\n",
      "+---+---------+\n",
      "| id|dept_name|\n",
      "+---+---------+\n",
      "|  1|       HR|\n",
      "|  2|       IT|\n",
      "|  3|    Sales|\n",
      "|  4|Marketing|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_data = [(101, \"Paul\", 32, 2), (102, \"Tina\", 45, 3), (103, \"John\", 28, 2), (104, \"Mike\", 36, 1), (105, \"David\", 34, 2), (106, \"Wright\", 25, 5)]\n",
    "dept_data = [(1, \"HR\"), (2, \"IT\"), (3, \"Sales\"), (4, \"Marketing\")]\n",
    "\n",
    "emp_df = spark.createDataFrame(data=emp_data, schema=[\"id\", \"emp_name\", \"age\", \"dept_id\"])\n",
    "dept_df = spark.createDataFrame(data=dept_data, schema=[\"id\", \"dept_name\"])\n",
    "\n",
    "emp_df.show()\n",
    "dept_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-------+---+---------+\n",
      "| id|emp_name|age|dept_id| id|dept_name|\n",
      "+---+--------+---+-------+---+---------+\n",
      "|104|    Mike| 36|      1|  1|       HR|\n",
      "|101|    Paul| 32|      2|  2|       IT|\n",
      "|103|    John| 28|      2|  2|       IT|\n",
      "|105|   David| 34|      2|  2|       IT|\n",
      "|102|    Tina| 45|      3|  3|    Sales|\n",
      "+---+--------+---+-------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.join(other=dept_df, on=(emp_df.dept_id == dept_df.id), how=\"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-------+----+---------+\n",
      "| id|emp_name|age|dept_id|  id|dept_name|\n",
      "+---+--------+---+-------+----+---------+\n",
      "|101|    Paul| 32|      2|   2|       IT|\n",
      "|102|    Tina| 45|      3|   3|    Sales|\n",
      "|103|    John| 28|      2|   2|       IT|\n",
      "|104|    Mike| 36|      1|   1|       HR|\n",
      "|105|   David| 34|      2|   2|       IT|\n",
      "|106|  Wright| 25|      5|NULL|     NULL|\n",
      "+---+--------+---+-------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.join(other=dept_df, on=(emp_df.dept_id == dept_df.id), how=\"left\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+----+-------+---+---------+\n",
      "|  id|emp_name| age|dept_id| id|dept_name|\n",
      "+----+--------+----+-------+---+---------+\n",
      "| 104|    Mike|  36|      1|  1|       HR|\n",
      "| 105|   David|  34|      2|  2|       IT|\n",
      "| 103|    John|  28|      2|  2|       IT|\n",
      "| 101|    Paul|  32|      2|  2|       IT|\n",
      "| 102|    Tina|  45|      3|  3|    Sales|\n",
      "|NULL|    NULL|NULL|   NULL|  4|Marketing|\n",
      "+----+--------+----+-------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.join(other=dept_df, on=(emp_df.dept_id == dept_df.id), how=\"right\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+----+-------+----+---------+\n",
      "|  id|emp_name| age|dept_id|  id|dept_name|\n",
      "+----+--------+----+-------+----+---------+\n",
      "| 104|    Mike|  36|      1|   1|       HR|\n",
      "| 101|    Paul|  32|      2|   2|       IT|\n",
      "| 103|    John|  28|      2|   2|       IT|\n",
      "| 105|   David|  34|      2|   2|       IT|\n",
      "| 102|    Tina|  45|      3|   3|    Sales|\n",
      "|NULL|    NULL|NULL|   NULL|   4|Marketing|\n",
      "| 106|  Wright|  25|      5|NULL|     NULL|\n",
      "+----+--------+----+-------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.join(other=dept_df, on=(emp_df.dept_id == dept_df.id), how=\"full\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### leftsemi join\n",
    "Similar to inner join. Returns matched records from both left and right dataframes but with only left dataframe columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-------+\n",
      "| id|emp_name|age|dept_id|\n",
      "+---+--------+---+-------+\n",
      "|104|    Mike| 36|      1|\n",
      "|101|    Paul| 32|      2|\n",
      "|103|    John| 28|      2|\n",
      "|105|   David| 34|      2|\n",
      "|102|    Tina| 45|      3|\n",
      "+---+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.join(other=dept_df, on=(emp_df.dept_id == dept_df.id), how=\"leftsemi\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### leftanti join\n",
    "Opposite of leftsemi join. Returns unmatched records from of left dataframe with only left dataframe columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+-------+\n",
      "| id|emp_name|age|dept_id|\n",
      "+---+--------+---+-------+\n",
      "|106|  Wright| 25|      5|\n",
      "+---+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.join(other=dept_df, on=(emp_df.dept_id == dept_df.id), how=\"leftanti\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### self join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+----------+\n",
      "| id|emp_name|age|manager_id|\n",
      "+---+--------+---+----------+\n",
      "|101|    Paul| 32|       102|\n",
      "|102|    Tina| 45|       100|\n",
      "|103|    John| 28|       105|\n",
      "|104|    Mike| 36|       102|\n",
      "|105|   David| 34|       100|\n",
      "|106|  Wright| 25|       105|\n",
      "+---+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_data = [(101, \"Paul\", 32, 102), (102, \"Tina\", 45, 100), (103, \"John\", 28, 105), (104, \"Mike\", 36, 102), (105, \"David\", 34, 100), (106, \"Wright\", 25, 105)]\n",
    "\n",
    "emp_df = spark.createDataFrame(data=emp_data, schema=[\"id\", \"emp_name\", \"age\", \"manager_id\"])\n",
    "\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------+\n",
      "|emp_id|emp_name|manager_name|\n",
      "+------+--------+------------+\n",
      "|   101|    Paul|        Tina|\n",
      "|   104|    Mike|        Tina|\n",
      "|   103|    John|       David|\n",
      "|   106|  Wright|       David|\n",
      "+------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "emp_df.alias(\"emp\").join(other=emp_df.alias(\"manager\"), on=(col(\"emp.manager_id\") == col(\"manager.id\")), how=\"inner\") \\\n",
    "    .select([col(\"emp.id\").alias(\"emp_id\"), col(\"emp.emp_name\").alias(\"emp_name\"), col(\"manager.emp_name\").alias(\"manager_name\")]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------------+\n",
      "|emp_id|emp_name|manager_name|\n",
      "+------+--------+------------+\n",
      "|   101|    Paul|        Tina|\n",
      "|   102|    Tina|        NULL|\n",
      "|   103|    John|       David|\n",
      "|   104|    Mike|        Tina|\n",
      "|   105|   David|        NULL|\n",
      "|   106|  Wright|       David|\n",
      "+------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.alias(\"emp\").join(other=emp_df.alias(\"manager\"), on=(col(\"emp.manager_id\") == col(\"manager.id\")), how=\"left\") \\\n",
    "    .select([col(\"emp.id\").alias(\"emp_id\"), col(\"emp.emp_name\").alias(\"emp_name\"), col(\"manager.emp_name\").alias(\"manager_name\")]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pivot()\n",
    "Rotate the row data (column having discrete/categorical values) in to individual columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+----------+\n",
      "| id| name|salary|department|\n",
      "+---+-----+------+----------+\n",
      "|  1| Paul| 32000|        HR|\n",
      "|  2| Tina| 45000|        HR|\n",
      "|  3| John| 28000|        IT|\n",
      "|  4| Mike| 36000|        IT|\n",
      "|  5|David| 34000|     Sales|\n",
      "+---+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"Paul\", 32000, \"HR\"), (2, \"Tina\", 45000, \"HR\"), (3, \"John\", 28000, \"IT\"), (4, \"Mike\", 36000, \"IT\"), (5, \"David\", 34000, \"Sales\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=[\"id\", \"name\", \"salary\", \"department\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+-----+\n",
      "|department|  HR|  IT|Sales|\n",
      "+----------+----+----+-----+\n",
      "|        HR|   2|NULL| NULL|\n",
      "|        IT|NULL|   2| NULL|\n",
      "|     Sales|NULL|NULL|    1|\n",
      "+----------+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(df.department).pivot(\"department\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+-----+\n",
      "|department|   HR|   IT|Sales|\n",
      "+----------+-----+-----+-----+\n",
      "|        HR|45000| NULL| NULL|\n",
      "|        IT| NULL|36000| NULL|\n",
      "|     Sales| NULL| NULL|34000|\n",
      "+----------+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(df.department).pivot(\"department\", [\"HR\", \"IT\", \"Sales\"]).max(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unpivot()\n",
    "- Available from pyspark 3.4.X onwards\n",
    "- Rotate the columns to row data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "| ID|Month1|Month2|Month3|\n",
      "+---+------+------+------+\n",
      "|  1|   100|   200|   300|\n",
      "|  2|   400|   500|   600|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, 100, 200, 300),\n",
    "    (2, 400, 500, 600)\n",
    "]\n",
    "schema = [\"ID\", \"Month1\", \"Month2\", \"Month3\"]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "| ID| Month|Value|\n",
      "+---+------+-----+\n",
      "|  1|Month1|  100|\n",
      "|  1|Month2|  200|\n",
      "|  1|Month3|  300|\n",
      "|  2|Month1|  400|\n",
      "|  2|Month2|  500|\n",
      "|  2|Month3|  600|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.unpivot(ids=\"ID\", values=[\"Month1\", \"Month2\", \"Month3\"], variableColumnName=\"Month\", valueColumnName=\"Value\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fill() or fillna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+----------+\n",
      "| id| name|salary|department|\n",
      "+---+-----+------+----------+\n",
      "|  1| Paul| 32000|      NULL|\n",
      "|  2| Tina| 45000|        HR|\n",
      "|  3| John| 28000|      NULL|\n",
      "|  4| Mike|  NULL|        IT|\n",
      "|  5|David| 34000|     Sales|\n",
      "+---+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"Paul\", 32000, None), (2, \"Tina\", 45000, \"HR\"), (3, \"John\", 28000, None), (4, \"Mike\", None, \"IT\"), (5, \"David\", 34000, \"Sales\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=[\"id\", \"name\", \"salary\", \"department\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+----------+\n",
      "| id| name|salary|department|\n",
      "+---+-----+------+----------+\n",
      "|  1| Paul| 32000|        IT|\n",
      "|  2| Tina| 45000|        HR|\n",
      "|  3| John| 28000|        IT|\n",
      "|  4| Mike|  NULL|        IT|\n",
      "|  5|David| 34000|     Sales|\n",
      "+---+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.fillna(\"IT\", [\"department\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+----------+\n",
      "| id| name|salary|department|\n",
      "+---+-----+------+----------+\n",
      "|  1| Paul| 32000|      NULL|\n",
      "|  2| Tina| 45000|        HR|\n",
      "|  3| John| 28000|      NULL|\n",
      "|  4| Mike|     0|        IT|\n",
      "|  5|David| 34000|     Sales|\n",
      "+---+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.fillna(0, [\"salary\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+----------+\n",
      "| id| name|salary|department|\n",
      "+---+-----+------+----------+\n",
      "|  1| Paul| 32000|      NULL|\n",
      "|  2| Tina| 45000|        HR|\n",
      "|  3| John| 28000|      NULL|\n",
      "|  4| Mike|     0|        IT|\n",
      "|  5|David| 34000|     Sales|\n",
      "+---+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(0, [\"salary\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|avg_salary|\n",
      "+----------+\n",
      "|   34750.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df.select(avg(df[\"salary\"]).alias(\"avg_salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34750.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "avg_salary = df.select(avg(df[\"salary\"]).alias(\"avg_salary\")).collect()[0].avg_salary\n",
    "avg_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+----------+\n",
      "| id| name|salary|department|\n",
      "+---+-----+------+----------+\n",
      "|  1| Paul| 32000|      NULL|\n",
      "|  2| Tina| 45000|        HR|\n",
      "|  3| John| 28000|      NULL|\n",
      "|  4| Mike| 34750|        IT|\n",
      "|  5|David| 34000|     Sales|\n",
      "+---+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.fillna(avg_salary, [\"salary\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample()\n",
    "- Return random subset of records from dataframe\n",
    "- fraction indicates percentage of records to return\n",
    "- seed indicates to return same subset of records each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  3|\n",
      "|  5|\n",
      "|  7|\n",
      "|  9|\n",
      "| 11|\n",
      "| 13|\n",
      "| 15|\n",
      "| 17|\n",
      "| 19|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(start=1, end=20, step=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+----------+\n",
      "| id| name|salary|department|\n",
      "+---+-----+------+----------+\n",
      "|  1| Paul| 32000|      NULL|\n",
      "|  3| John| 28000|      NULL|\n",
      "|  5|David| 34000|     Sales|\n",
      "+---+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sample(fraction=0.2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+----------+\n",
      "| id| name|salary|department|\n",
      "+---+-----+------+----------+\n",
      "|  5|David| 34000|     Sales|\n",
      "+---+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sample(fraction=0.2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+----------+\n",
      "| id| name|salary|department|\n",
      "+---+-----+------+----------+\n",
      "|  3| John| 28000|      NULL|\n",
      "|  5|David| 34000|     Sales|\n",
      "+---+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sample(fraction=0.2, seed=123).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+----------+\n",
      "| id| name|salary|department|\n",
      "+---+-----+------+----------+\n",
      "|  3| John| 28000|      NULL|\n",
      "|  5|David| 34000|     Sales|\n",
      "+---+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sample(fraction=0.2, seed=123).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### collect()\n",
    "\n",
    "- Returns the rows/records of the dataframe.\n",
    "- The output type is list of Row() objects\n",
    "- Basically, collect function collects/gathers the data from all the partitions. So, don't use collect() for large dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+----------+\n",
      "| id| name|salary|department|\n",
      "+---+-----+------+----------+\n",
      "|  1| Paul| 32000|      NULL|\n",
      "|  2| Tina| 45000|        HR|\n",
      "|  3| John| 28000|      NULL|\n",
      "|  4| Mike|  NULL|        IT|\n",
      "|  5|David| 34000|     Sales|\n",
      "+---+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, name='Paul', salary=32000),\n",
       " Row(id=2, name='Tina', salary=45000),\n",
       " Row(id=3, name='John', salary=28000),\n",
       " Row(id=4, name='Mike', salary=None),\n",
       " Row(id=5, name='David', salary=34000)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = df.select(df.id, df.name, df.salary).collect()\n",
    "records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 John 34000\n"
     ]
    }
   ],
   "source": [
    "print(records[0].id, records[2].name, records[4].salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id: 1, Name: Paul, Salary: 32000\n",
      "Id: 2, Name: Tina, Salary: 45000\n",
      "Id: 3, Name: John, Salary: 28000\n",
      "Id: 4, Name: Mike, Salary: None\n",
      "Id: 5, Name: David, Salary: 34000\n"
     ]
    }
   ],
   "source": [
    "for row in records:\n",
    "    print(f\"Id: {row.id}, Name: {row.name}, Salary: {row.salary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform()\n",
    "\n",
    "Apply custom transformations to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+----------+\n",
      "| id| name|salary|department|\n",
      "+---+-----+------+----------+\n",
      "|  1| Paul| 32000|        HR|\n",
      "|  2| Tina| 45000|        HR|\n",
      "|  3| John| 28000|        IT|\n",
      "|  4| Mike| 36000|        IT|\n",
      "|  5|David| 34000|     Sales|\n",
      "+---+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"Paul\", 32000, \"HR\"), (2, \"Tina\", 45000, \"HR\"), (3, \"John\", 28000, \"IT\"), (4, \"Mike\", 36000, \"IT\"), (5, \"David\", 34000, \"Sales\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=[\"id\", \"name\", \"salary\", \"department\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+----------+\n",
      "| id| name|salary|department|\n",
      "+---+-----+------+----------+\n",
      "|  1| PAUL| 32000|        HR|\n",
      "|  2| TINA| 45000|        HR|\n",
      "|  3| JOHN| 28000|        IT|\n",
      "|  4| MIKE| 36000|        IT|\n",
      "|  5|DAVID| 34000|     Sales|\n",
      "+---+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Built-in transformation\n",
    "from pyspark.sql.functions import upper\n",
    "\n",
    "df.withColumn(\"name\", (upper(df.name))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+----------+\n",
      "| id| name| salary|department|\n",
      "+---+-----+-------+----------+\n",
      "|  1| Paul|48000.0|        HR|\n",
      "|  2| Tina|67500.0|        HR|\n",
      "|  3| John|42000.0|        IT|\n",
      "|  4| Mike|54000.0|        IT|\n",
      "|  5|David|51000.0|     Sales|\n",
      "+---+-----+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Custom transformation function\n",
    "def increment_salary(df):\n",
    "    return df.withColumn(\"salary\", (df.salary + (df.salary * 0.5)))\n",
    "\n",
    "df.transform(increment_salary).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column transform()\n",
    "- Import as: from pyspark.sql.functions import transform\n",
    "- Apply transformation on array type column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------+\n",
      "|order_id|items                  |\n",
      "+--------+-----------------------+\n",
      "|100     |[PC, Monitor, Keyboard]|\n",
      "|101     |[Laptop, Speaker]      |\n",
      "|102     |[Mouse, Adapter]       |\n",
      "|103     |[Headphone]            |\n",
      "+--------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(100, [\"PC\", \"Monitor\", \"Keyboard\"]), (101, [\"Laptop\", \"Speaker\"]), (102, [\"Mouse\", \"Adapter\"]), (103, [\"Headphone\"])]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(name=\"order_id\", dataType=IntegerType(), nullable=False),\n",
    "    StructField(name=\"items\", dataType=ArrayType(elementType=StringType()), nullable=False)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------+\n",
      "|order_id|items                  |\n",
      "+--------+-----------------------+\n",
      "|100     |[PC, MONITOR, KEYBOARD]|\n",
      "|101     |[LAPTOP, SPEAKER]      |\n",
      "|102     |[MOUSE, ADAPTER]       |\n",
      "|103     |[HEADPHONE]            |\n",
      "+--------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import transform\n",
    "\n",
    "df.select(df.order_id, transform(col=\"items\", f=(lambda item: upper(item))).alias(\"items\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### createOrReplaceTempView()\n",
    "- Create or replace the temporary view from dataframe\n",
    "- Can be used to execure raw SQL queries\n",
    "- Available only with-in the current spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+----------+\n",
      "| id| name|salary|department|\n",
      "+---+-----+------+----------+\n",
      "|  1| Paul| 32000|        HR|\n",
      "|  2| Tina| 45000|        HR|\n",
      "|  3| John| 28000|        IT|\n",
      "|  4| Mike| 36000|        IT|\n",
      "|  5|David| 34000|     Sales|\n",
      "+---+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"Paul\", 32000, \"HR\"), (2, \"Tina\", 45000, \"HR\"), (3, \"John\", 28000, \"IT\"), (4, \"Mike\", 36000, \"IT\"), (5, \"David\", 34000, \"Sales\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=[\"id\", \"name\", \"salary\", \"department\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"employee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+----------+\n",
      "| id| name|salary|department|\n",
      "+---+-----+------+----------+\n",
      "|  1| Paul| 32000|        HR|\n",
      "|  2| Tina| 45000|        HR|\n",
      "|  3| John| 28000|        IT|\n",
      "|  4| Mike| 36000|        IT|\n",
      "|  5|David| 34000|     Sales|\n",
      "+---+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM employee;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+\n",
      "| id| name|department|\n",
      "+---+-----+----------+\n",
      "|  1| Paul|        HR|\n",
      "|  2| Tina|        HR|\n",
      "|  3| John|        IT|\n",
      "|  4| Mike|        IT|\n",
      "|  5|David|     Sales|\n",
      "+---+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT id, name, department FROM employee;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### createOrReplaceGlobalTempView\n",
    "- Available across all spark sessions\n",
    "- Global temporary view is available in global_temp schema/database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceGlobalTempView(\"employee_global\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+\n",
      "| id| name|department|\n",
      "+---+-----+----------+\n",
      "|  1| Paul|        HR|\n",
      "|  2| Tina|        HR|\n",
      "|  3| John|        IT|\n",
      "|  4| Mike|        IT|\n",
      "|  5|David|     Sales|\n",
      "+---+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT id, name, department FROM global_temp.employee_global;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_2 = SparkSession.builder \\\n",
    "    .appName(\"SparkApp-Session-2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+\n",
      "| id| name|department|\n",
      "+---+-----+----------+\n",
      "|  1| Paul|        HR|\n",
      "|  2| Tina|        HR|\n",
      "|  3| John|        IT|\n",
      "|  4| Mike|        IT|\n",
      "|  5|David|     Sales|\n",
      "+---+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_2.sql(\"SELECT id, name, department FROM global_temp.employee_global;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
