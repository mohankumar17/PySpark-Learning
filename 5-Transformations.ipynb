{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkApp-DF-Tranformations\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://MSI.bbrouter:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkApp-DF-Tranformations</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2608fa07730>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"resources/in/employee/employee_data_1.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------+\n",
      "| ID|        Name| Department|Salary|\n",
      "+---+------------+-----------+------+\n",
      "|  1|    John Doe|Engineering| 50000|\n",
      "|  2|  Jane Smith|  Marketing| 45000|\n",
      "|  3|   Jim Brown|      Sales| 40000|\n",
      "|  4|Jackie White|         HR| 42000|\n",
      "|  5| Emily Davis|Engineering| 60000|\n",
      "+---+------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### withColumn()\n",
    "\n",
    "Apply transformations to a column of dataframe. It return new dataframe.\n",
    "\n",
    "1. Convert the datatypes of columns\n",
    "2. Create new columns or replace the existing columns\n",
    "3. Transform entire columns with values\n",
    "4. Concate the columns etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(colName=\"Salary\", col=df.Salary.cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+-------+\n",
      "| ID|        Name| Department| Salary|\n",
      "+---+------------+-----------+-------+\n",
      "|  1|    John Doe|Engineering|50000.0|\n",
      "|  2|  Jane Smith|  Marketing|45000.0|\n",
      "|  3|   Jim Brown|      Sales|40000.0|\n",
      "|  4|Jackie White|         HR|42000.0|\n",
      "|  5| Emily Davis|Engineering|60000.0|\n",
      "+---+------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(colName=\"Bonus_Salary\", col=df.Salary * 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+-------+------------+\n",
      "| ID|        Name| Department| Salary|Bonus_Salary|\n",
      "+---+------------+-----------+-------+------------+\n",
      "|  1|    John Doe|Engineering|50000.0|     12500.0|\n",
      "|  2|  Jane Smith|  Marketing|45000.0|     11250.0|\n",
      "|  3|   Jim Brown|      Sales|40000.0|     10000.0|\n",
      "|  4|Jackie White|         HR|42000.0|     10500.0|\n",
      "|  5| Emily Davis|Engineering|60000.0|     15000.0|\n",
      "+---+------------+-----------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(colName=\"Total_Salary\", col=df.Salary + df.Bonus_Salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+-------+------------+------------+\n",
      "| ID|        Name| Department| Salary|Bonus_Salary|Total_Salary|\n",
      "+---+------------+-----------+-------+------------+------------+\n",
      "|  1|    John Doe|Engineering|50000.0|     12500.0|     62500.0|\n",
      "|  2|  Jane Smith|  Marketing|45000.0|     11250.0|     56250.0|\n",
      "|  3|   Jim Brown|      Sales|40000.0|     10000.0|     50000.0|\n",
      "|  4|Jackie White|         HR|42000.0|     10500.0|     52500.0|\n",
      "|  5| Emily Davis|Engineering|60000.0|     15000.0|     75000.0|\n",
      "+---+------------+-----------+-------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### withColumnRenamed()\n",
    "\n",
    "Rename the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(existing=\"Name\", new=\"Emp_Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+-------+------------+------------+\n",
      "| ID|  Emp_Name| Department| Salary|Bonus_Salary|Total_Salary|\n",
      "+---+----------+-----------+-------+------------+------------+\n",
      "|  1|  John Doe|Engineering|50000.0|     12500.0|     62500.0|\n",
      "|  2|Jane Smith|  Marketing|45000.0|     11250.0|     56250.0|\n",
      "|  3| Jim Brown|      Sales|40000.0|     10000.0|     50000.0|\n",
      "+---+----------+-----------+-------+------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop()\n",
    "Delete or remove the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"Bonus_Salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+-------+------------+\n",
      "| ID|  Emp_Name| Department| Salary|Total_Salary|\n",
      "+---+----------+-----------+-------+------------+\n",
      "|  1|  John Doe|Engineering|50000.0|     62500.0|\n",
      "|  2|Jane Smith|  Marketing|45000.0|     56250.0|\n",
      "|  3| Jim Brown|      Sales|40000.0|     50000.0|\n",
      "+---+----------+-----------+-------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array Functions\n",
    "1. explode()\n",
    "2. split()\n",
    "3. array()\n",
    "4. array_contains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(100, [\"PC\", \"Monitor\", \"Keyboard\"]), (101, [\"Laptop\", \"Speaker\"]), (102, [\"Mouse\", \"Adapter\"]), (103, [\"Headphone\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = false)\n",
      " |-- items: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(name=\"order_id\", dataType=IntegerType(), nullable=False),\n",
    "    StructField(name=\"items\", dataType=ArrayType(elementType=StringType()), nullable=False)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------+\n",
      "|order_id|items                  |\n",
      "+--------+-----------------------+\n",
      "|100     |[PC, Monitor, Keyboard]|\n",
      "|101     |[Laptop, Speaker]      |\n",
      "|102     |[Mouse, Adapter]       |\n",
      "|103     |[Headphone]            |\n",
      "+--------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|order_id|     item|\n",
      "+--------+---------+\n",
      "|     100|       PC|\n",
      "|     100|  Monitor|\n",
      "|     100| Keyboard|\n",
      "|     101|   Laptop|\n",
      "|     101|  Speaker|\n",
      "|     102|    Mouse|\n",
      "|     102|  Adapter|\n",
      "|     103|Headphone|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df = df.withColumn(\"item\", explode(df.items))\n",
    "df = df.drop(\"items\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- items: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(100, \"PC,Monitor,Keyboard\"), (101, \"Laptop,Speaker\"), (102, \"Mouse,Adapter\"), (103, \"Headphone\")]\n",
    "df = spark.createDataFrame(data=data, schema=[\"order_id\", \"items\"])\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|order_id|              items|\n",
      "+--------+-------------------+\n",
      "|     100|PC,Monitor,Keyboard|\n",
      "|     101|     Laptop,Speaker|\n",
      "|     102|      Mouse,Adapter|\n",
      "|     103|          Headphone|\n",
      "+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "df = df.withColumn(\"items\", split(df.items, \",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------+\n",
      "|order_id|items                  |\n",
      "+--------+-----------------------+\n",
      "|100     |[PC, Monitor, Keyboard]|\n",
      "|101     |[Laptop, Speaker]      |\n",
      "|102     |[Mouse, Adapter]       |\n",
      "|103     |[Headphone]            |\n",
      "+--------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- country_codes: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(100, [\"US\", \"UK\", \"AUS\"]), (101, [\"US\", \"UK\"]), (102, [\"IND\", \"UK\"]), (103, [\"IND\"])]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=[\"id\", \"country_codes\"])\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-------+\n",
      "| id|country_codes|isLocal|\n",
      "+---+-------------+-------+\n",
      "|100|[US, UK, AUS]|   true|\n",
      "|101|     [US, UK]|   true|\n",
      "|102|    [IND, UK]|  false|\n",
      "|103|        [IND]|  false|\n",
      "+---+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains, col\n",
    "\n",
    "df = df.withColumn(\"isLocal\", array_contains(col(\"country_codes\"), \"US\"))\n",
    "#df = df.withColumn(\"isLocal\", array_contains(df.country_codes, \"US\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+\n",
      "| id|first_name|last_name|\n",
      "+---+----------+---------+\n",
      "|100|      Paul|  Brandon|\n",
      "|101|      John|      Doe|\n",
      "|102|      Tina|   Nailor|\n",
      "+---+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(100, \"Paul\", \"Brandon\"), (101, \"John\", \"Doe\"), (102, \"Tina\", \"Nailor\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=[\"id\", \"first_name\", \"last_name\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+---------------+\n",
      "| id|first_name|last_name|      full_name|\n",
      "+---+----------+---------+---------------+\n",
      "|100|      Paul|  Brandon|[Paul, Brandon]|\n",
      "|101|      John|      Doe|    [John, Doe]|\n",
      "|102|      Tina|   Nailor| [Tina, Nailor]|\n",
      "+---+----------+---------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array\n",
    "\n",
    "df = df.withColumn(\"full_name\", array(df.first_name, df.last_name))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Functions\n",
    "1. explode()\n",
    "2. map_keys()\n",
    "3. map_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------------------+\n",
      "|user_id|users_info                      |\n",
      "+-------+--------------------------------+\n",
      "|1      |{gender -> male, name -> Paul}  |\n",
      "|2      |{gender -> female, name -> Tina}|\n",
      "|3      |{gender -> male, name -> John}  |\n",
      "+-------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1, {\"name\": \"Paul\", \"gender\": \"male\"}), (2, {\"name\": \"Tina\", \"gender\": \"female\"}), (3, {\"name\": \"John\", \"gender\": \"male\"})]\n",
    "\n",
    "from pyspark.sql.types import StringType, IntegerType, MapType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(name=\"user_id\", dataType=IntegerType()),\n",
    "    StructField(name=\"users_info\", dataType=MapType(keyType=StringType(), valueType=StringType()))\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------------------+-----------+\n",
      "|user_id|users_info                      |user_gender|\n",
      "+-------+--------------------------------+-----------+\n",
      "|1      |{gender -> male, name -> Paul}  |male       |\n",
      "|2      |{gender -> female, name -> Tina}|female     |\n",
      "|3      |{gender -> male, name -> John}  |male       |\n",
      "+-------+--------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"user_gender\", df.users_info.gender)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|user_id|user_gender|\n",
      "+-------+-----------+\n",
      "|      1|       male|\n",
      "|      2|     female|\n",
      "|      3|       male|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"user_id\", \"user_gender\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|user_id|   key| value|\n",
      "+-------+------+------+\n",
      "|      1|gender|  male|\n",
      "|      1|  name|  Paul|\n",
      "|      2|gender|female|\n",
      "|      2|  name|  Tina|\n",
      "|      3|gender|  male|\n",
      "|      3|  name|  John|\n",
      "+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df.select(\"user_id\", explode(df.users_info)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------+\n",
      "|user_id|          users_info|user_gender|\n",
      "+-------+--------------------+-----------+\n",
      "|      1|{gender -> male, ...|       male|\n",
      "|      2|{gender -> female...|     female|\n",
      "|      3|{gender -> male, ...|       male|\n",
      "+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------+--------------+\n",
      "|user_id|          users_info|user_gender|     info_keys|\n",
      "+-------+--------------------+-----------+--------------+\n",
      "|      1|{gender -> male, ...|       male|[gender, name]|\n",
      "|      2|{gender -> female...|     female|[gender, name]|\n",
      "|      3|{gender -> male, ...|       male|[gender, name]|\n",
      "+-------+--------------------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_keys\n",
    "\n",
    "df.withColumn(\"info_keys\", map_keys(df.users_info)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------+--------------+\n",
      "|user_id|          users_info|user_gender|   info_values|\n",
      "+-------+--------------------+-----------+--------------+\n",
      "|      1|{gender -> male, ...|       male|  [male, Paul]|\n",
      "|      2|{gender -> female...|     female|[female, Tina]|\n",
      "|      3|{gender -> male, ...|       male|  [male, John]|\n",
      "+-------+--------------------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_values\n",
    "\n",
    "df.withColumn(\"info_values\", map_values(df.users_info)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
