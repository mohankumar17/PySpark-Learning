{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame\n",
    "\n",
    "A DataFrame is a distributed collection of data in a strucured way. The data is organized as rows and named columns.\n",
    "\n",
    "1. DataFrame is abstracted RDD.\n",
    "3. It is similar to table in realtional (SQL) database.\n",
    "3. Enables query and other optimizations.\n",
    "\n",
    "### Advantages\n",
    "1. Optimized Execution\n",
    "2. Ease of Use\n",
    "3. Integration with Eco-System: Integrated easily with MLlib, Streaming and other extensions\n",
    "4. Built-in Optimization\n",
    "5. Powerful Data Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySparkApp-DataFrame\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://MSI.bbrouter:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MySparkApp-DataFrame</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x205c3dd31f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1, \"Paul\", 32), (2, \"Tina\", 45), (3, \"John\", 28)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "| _1|  _2| _3|\n",
      "+---+----+---+\n",
      "|  1|Paul| 32|\n",
      "|  2|Tina| 45|\n",
      "|  3|John| 28|\n",
      "+---+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data=data, schema=[\"id\", \"name\", \"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "| id|name|age|\n",
      "+---+----+---+\n",
      "|  1|Paul| 32|\n",
      "|  2|Tina| 45|\n",
      "|  3|John| 28|\n",
      "+---+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame with External Schema\n",
    "\n",
    "1. StructType\n",
    "2. StructField\n",
    "3. Data Types: LongType, StringType, IntegerType, ArrayType, MapType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- name: string (nullable = false)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(name=\"id\", dataType=LongType(), nullable=False),\n",
    "    StructField(name=\"name\", dataType=StringType(), nullable=False),\n",
    "    StructField(name=\"age\", dataType=IntegerType(), nullable=True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "| id|name|age|\n",
      "+---+----+---+\n",
      "|  1|Paul| 32|\n",
      "|  2|Tina| 45|\n",
      "|  3|John| 28|\n",
      "+---+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(100, [\"PC\", \"Monitor\", \"Keyboard\"]), (101, [\"Laptop\", \"Speaker\"]), (102, [\"Mouse\", \"Adapter\"]), (103, [\"Headphone\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = false)\n",
      " |-- items: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, ArrayType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(name=\"order_id\", dataType=IntegerType(), nullable=False),\n",
    "    StructField(name=\"items\", dataType=ArrayType(elementType=StringType()), nullable=False)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------+\n",
      "|order_id|items                  |\n",
      "+--------+-----------------------+\n",
      "|100     |[PC, Monitor, Keyboard]|\n",
      "|101     |[Laptop, Speaker]      |\n",
      "|102     |[Mouse, Adapter]       |\n",
      "|103     |[Headphone]            |\n",
      "+--------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MapType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1, {\"name\": \"Paul\", \"gender\": \"male\"}), (2, {\"name\": \"Tina\", \"gender\": \"female\"}), (3, {\"name\": \"John\", \"gender\": \"male\"})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- users_info: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, MapType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(name=\"user_id\", dataType=IntegerType()),\n",
    "    StructField(name=\"users_info\", dataType=MapType(keyType=StringType(), valueType=StringType()))\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------------------+\n",
      "|user_id|users_info                      |\n",
      "+-------+--------------------------------+\n",
      "|1      |{gender -> male, name -> Paul}  |\n",
      "|2      |{gender -> female, name -> Tina}|\n",
      "|3      |{gender -> male, name -> John}  |\n",
      "+-------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"name\": \"Paul\",\n",
    "        \"age\": 32\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"name\": \"Tina\",\n",
    "        \"age\": 45\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"name\": \"John\",\n",
    "        \"age\": 28\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "|age| id|name|\n",
      "+---+---+----+\n",
      "| 32|  1|Paul|\n",
      "| 45|  2|Tina|\n",
      "| 28|  3|John|\n",
      "+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data=data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading CSV File to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"resources/in/employee/employee_data_1.csv\", header=True, inferSchema=True)\n",
    "#df = spark.read.format(\"csv\").load(\"resources/in/employee_data_1.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### show() - Print or display the DataFrame records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------+------+\n",
      "| ID|          Name| Department|Salary|\n",
      "+---+--------------+-----------+------+\n",
      "|  1|      John Doe|Engineering| 50000|\n",
      "|  2|    Jane Smith|  Marketing| 45000|\n",
      "|  3|     Jim Brown|      Sales| 40000|\n",
      "|  4|  Jackie White|         HR| 42000|\n",
      "|  5|   Emily Davis|Engineering| 60000|\n",
      "|  6| Michael Scott| Management| 75000|\n",
      "|  7|    Pam Beesly|  Reception| 35000|\n",
      "|  8|Dwight Schrute|      Sales| 50000|\n",
      "|  9| Angela Martin| Accounting| 48000|\n",
      "| 10|  Kevin Malone| Accounting| 45000|\n",
      "| 11|Oscar Martinez| Accounting| 47000|\n",
      "| 12|Stanley Hudson|      Sales| 46000|\n",
      "+---+--------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show() # Display first 20 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------+\n",
      "| ID|        Name| Department|Salary|\n",
      "+---+------------+-----------+------+\n",
      "|  1|    John Doe|Engineering| 50000|\n",
      "|  2|  Jane Smith|  Marketing| 45000|\n",
      "|  3|   Jim Brown|      Sales| 40000|\n",
      "|  4|Jackie White|         HR| 42000|\n",
      "|  5| Emily Davis|Engineering| 60000|\n",
      "+---+------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5) # Display first N rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+------+\n",
      "| ID|      Name|Department|Salary|\n",
      "+---+----------+----------+------+\n",
      "|  1|  John Doe|Enginee...| 50000|\n",
      "|  2|Jane Smith| Marketing| 45000|\n",
      "|  3| Jim Brown|     Sales| 40000|\n",
      "|  4|Jackie ...|        HR| 42000|\n",
      "|  5|Emily D...|Enginee...| 60000|\n",
      "+---+----------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n=5, truncate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------\n",
      " ID         | 1           \n",
      " Name       | John Doe    \n",
      " Department | Engineering \n",
      " Salary     | 50000       \n",
      "-RECORD 1-----------------\n",
      " ID         | 2           \n",
      " Name       | Jane Smith  \n",
      " Department | Marketing   \n",
      " Salary     | 45000       \n",
      "-RECORD 2-----------------\n",
      " ID         | 3           \n",
      " Name       | Jim Brown   \n",
      " Department | Sales       \n",
      " Salary     | 40000       \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n=3, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading from multiple CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(path=[\"resources/in/employee/employee_data_1.csv\", \"resources/in/employee/employee_data_2.csv\"], header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-----------------+------+\n",
      "| ID|           Name|       Department|Salary|\n",
      "+---+---------------+-----------------+------+\n",
      "|  1|       John Doe|      Engineering| 50000|\n",
      "|  2|     Jane Smith|        Marketing| 45000|\n",
      "|  3|      Jim Brown|            Sales| 40000|\n",
      "|  4|   Jackie White|               HR| 42000|\n",
      "|  5|    Emily Davis|      Engineering| 60000|\n",
      "|  6|  Michael Scott|       Management| 75000|\n",
      "|  7|     Pam Beesly|        Reception| 35000|\n",
      "|  8| Dwight Schrute|            Sales| 50000|\n",
      "|  9|  Angela Martin|       Accounting| 48000|\n",
      "| 10|   Kevin Malone|       Accounting| 45000|\n",
      "| 11| Oscar Martinez|       Accounting| 47000|\n",
      "| 12| Stanley Hudson|            Sales| 46000|\n",
      "| 13|  Phyllis Vance|            Sales| 44000|\n",
      "| 14|    Ryan Howard|             Temp| 30000|\n",
      "| 15|   Kelly Kapoor| Customer Service| 37000|\n",
      "| 16|Toby Flenderson|               HR| 43000|\n",
      "| 17|  Creed Bratton|Quality Assurance| 38000|\n",
      "| 18|   Andy Bernard|            Sales| 49000|\n",
      "| 19|    Erin Hannon|        Reception| 34000|\n",
      "| 20| Darryl Philbin|        Warehouse| 41000|\n",
      "+---+---------------+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing DataFrame to CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------+------+\n",
      "| ID|          Name| Department|Salary|\n",
      "+---+--------------+-----------+------+\n",
      "|  1|      John Doe|Engineering| 50000|\n",
      "|  2|    Jane Smith|  Marketing| 45000|\n",
      "|  3|     Jim Brown|      Sales| 40000|\n",
      "|  4|  Jackie White|         HR| 42000|\n",
      "|  5|   Emily Davis|Engineering| 60000|\n",
      "|  6| Michael Scott| Management| 75000|\n",
      "|  7|    Pam Beesly|  Reception| 35000|\n",
      "|  8|Dwight Schrute|      Sales| 50000|\n",
      "|  9| Angela Martin| Accounting| 48000|\n",
      "| 10|  Kevin Malone| Accounting| 45000|\n",
      "| 11|Oscar Martinez| Accounting| 47000|\n",
      "| 12|Stanley Hudson|      Sales| 46000|\n",
      "+---+--------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"resources/in/employee/employee_data_1.csv\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df[\"Salary\"] >= 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------+------+\n",
      "| ID|          Name| Department|Salary|\n",
      "+---+--------------+-----------+------+\n",
      "|  1|      John Doe|Engineering| 50000|\n",
      "|  5|   Emily Davis|Engineering| 60000|\n",
      "|  6| Michael Scott| Management| 75000|\n",
      "|  8|Dwight Schrute|      Sales| 50000|\n",
      "+---+--------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download WinUtils from Hadoop\n",
    "\n",
    "- Clone and Extract the Repo for WinUtils: https://github.com/cdarlint/winutils\n",
    "- Resource: https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems\n",
    "- Set the environment variables as: HADOOP_HOME = Hadoop directory and Path = %HADOOP_HOME%\\bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.write.options(header='True', delimeter=\",\").csv(\"resources/out/high_paid_employee_data.csv\")\n",
    "#df.write.csv(path=\"hdfs://resources/out/employee_data\", mode=\"overwrite\", header=True, sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading from JSON File to DataFrame\n",
    "\n",
    "- Read from single and multiple JSON files\n",
    "- Read from line seperated JSON and multiline (JSON array) JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(path=\"resources/in/product_inventory/product_inventory_line_separated.json\", multiLine=False) #default multiline is False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---------+------------------+-----+\n",
      "|  Category|Price|ProductID|       ProductName|Stock|\n",
      "+----------+-----+---------+------------------+-----+\n",
      "|Smartphone|  999|     P001|   Apple iPhone 15|   50|\n",
      "|Smartphone|  899|     P002|Samsung Galaxy S23|   30|\n",
      "|Headphones|  299|     P003|   Sony WH-1000XM5|  100|\n",
      "|    Laptop| 1199|     P004|       Dell XPS 13|   20|\n",
      "|    Laptop| 1399|     P005|   HP Spectre x360|   15|\n",
      "+----------+-----+---------+------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Price: long (nullable = true)\n",
      " |-- ProductID: string (nullable = true)\n",
      " |-- ProductName: string (nullable = true)\n",
      " |-- Stock: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(path=\"resources/in/product_inventory/product_inventory_multiline.json\", multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---------+------------------+-----+\n",
      "|  Category|Price|ProductID|       ProductName|Stock|\n",
      "+----------+-----+---------+------------------+-----+\n",
      "|Smartphone|  999|     P001|   Apple iPhone 15|   50|\n",
      "|Smartphone|  899|     P002|Samsung Galaxy S23|   30|\n",
      "|Headphones|  299|     P003|   Sony WH-1000XM5|  100|\n",
      "|    Laptop| 1199|     P004|       Dell XPS 13|   20|\n",
      "|    Laptop| 1399|     P005|   HP Spectre x360|   15|\n",
      "+----------+-----+---------+------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(path=[\"resources/in/product_inventory/product_inventory_part1.json\", \"resources/in/product_inventory/product_inventory_part2.json\"], multiLine=True)\n",
    "#df = spark.read.json(path=\"resources/in/product_inventory/product_inventory_part*.json\", multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+---------+--------------------+-----+\n",
      "|      Category|Price|ProductID|         ProductName|Stock|\n",
      "+--------------+-----+---------+--------------------+-----+\n",
      "|    Smartphone|  999|     P001|     Apple iPhone 15|   50|\n",
      "|    Smartphone|  899|     P002|  Samsung Galaxy S23|   30|\n",
      "|    Headphones|  299|     P003|     Sony WH-1000XM5|  100|\n",
      "|        Laptop| 1199|     P004|         Dell XPS 13|   20|\n",
      "|        Laptop| 1399|     P005|     HP Spectre x360|   15|\n",
      "|    Headphones|  329|     P006|Bose QuietComfort 45|   80|\n",
      "|      Wearable|  499|     P007|Apple Watch Series 9|   60|\n",
      "|      Wearable|  399|     P008|Samsung Galaxy Wa...|   70|\n",
      "|      E-Reader|  129|     P009|   Kindle Paperwhite|  120|\n",
      "|   Accessories|   99|     P010|Logitech MX Master 3|   90|\n",
      "|   Accessories|   69|     P011| Razer DeathAdder V2|  100|\n",
      "|    Smartphone|  799|     P012|      Google Pixel 8|   40|\n",
      "|        Laptop| 1499|     P013|      Lenovo Yoga 9i|   25|\n",
      "|        Camera| 1999|     P014|         Sony A7 III|   10|\n",
      "|        Camera| 2499|     P015|        Canon EOS R6|    8|\n",
      "|         Drone|  799|     P016|     DJI Mavic Air 2|   15|\n",
      "|        Tablet|  999|     P017|Microsoft Surface...|   18|\n",
      "|        Tablet|  849|     P018|Samsung Galaxy Ta...|   20|\n",
      "|Gaming Console|  499|     P019|  Sony PlayStation 5|   50|\n",
      "|Gaming Console|  499|     P020|       Xbox Series X|   45|\n",
      "+--------------+-----+---------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing DataFrame to JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.write.json(path=\"resources/out/product_inventory\", mode=\"append\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading from Parquet files to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"resources/in/parquest_files/userdata1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----------+---------+--------------------+------+--------------+----------------+------------+---------+---------+--------------------+--------+\n",
      "|  registration_dttm| id|first_name|last_name|               email|gender|    ip_address|              cc|     country|birthdate|   salary|               title|comments|\n",
      "+-------------------+---+----------+---------+--------------------+------+--------------+----------------+------------+---------+---------+--------------------+--------+\n",
      "|2016-02-03 13:25:29|  1|    Amanda|   Jordan|    ajordan0@com.com|Female|   1.197.201.2|6759521864920116|   Indonesia| 3/8/1971| 49756.53|    Internal Auditor|   1E+02|\n",
      "|2016-02-03 22:34:03|  2|    Albert|  Freeman|     afreeman1@is.gd|  Male|218.111.175.34|                |      Canada|1/16/1968|150280.17|       Accountant IV|        |\n",
      "|2016-02-03 06:39:31|  3|    Evelyn|   Morgan|emorgan2@altervis...|Female|  7.161.136.94|6767119071901597|      Russia| 2/1/1960|144972.51| Structural Engineer|        |\n",
      "|2016-02-03 06:06:21|  4|    Denise|    Riley|    driley3@gmpg.org|Female| 140.35.109.83|3576031598965625|       China| 4/8/1997| 90263.05|Senior Cost Accou...|        |\n",
      "|2016-02-03 10:35:31|  5|    Carlos|    Burns|cburns4@miitbeian...|      |169.113.235.40|5602256255204850|South Africa|         |     NULL|                    |        |\n",
      "+-------------------+---+----------+---------+--------------------+------+--------------+----------------+------------+---------+---------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = spark.read.parquet(\"resources/in/parquest_files/*.parquet\")\n",
    "df = spark.read.parquet(*[\"resources/in/parquest_files/userdata1.parquet\", \"resources/in/parquest_files/userdata1.parquet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----------+---------+--------------------+------+--------------+----------------+---------+---------+---------+-------------------+--------+\n",
      "|  registration_dttm| id|first_name|last_name|               email|gender|    ip_address|              cc|  country|birthdate|   salary|              title|comments|\n",
      "+-------------------+---+----------+---------+--------------------+------+--------------+----------------+---------+---------+---------+-------------------+--------+\n",
      "|2016-02-03 13:25:29|  1|    Amanda|   Jordan|    ajordan0@com.com|Female|   1.197.201.2|6759521864920116|Indonesia| 3/8/1971| 49756.53|   Internal Auditor|   1E+02|\n",
      "|2016-02-03 22:34:03|  2|    Albert|  Freeman|     afreeman1@is.gd|  Male|218.111.175.34|                |   Canada|1/16/1968|150280.17|      Accountant IV|        |\n",
      "|2016-02-03 06:39:31|  3|    Evelyn|   Morgan|emorgan2@altervis...|Female|  7.161.136.94|6767119071901597|   Russia| 2/1/1960|144972.51|Structural Engineer|        |\n",
      "+-------------------+---+----------+---------+--------------------+------+--------------+----------------+---------+---------+---------+-------------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing DataFrame to Parquest File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.write.parquet(path=\"resources/out/parquest_files/\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### partitionBy()\n",
    "- Split the data into multiple small files based on specified columns.\n",
    "- Need to used carefully. It is recommeded to use on columns having discrete number of values. If used on continuous values like name/id, it create many folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.write.parquet(path=\"resources/out/parquest_files/user\", mode=\"overwrite\", partitionBy=[\"country\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.write.parquet(path=\"resources/out/parquest_files/user\", mode=\"overwrite\", partitionBy=[\"country\", \"gender\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Row()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "data = [Row(\"Laptop\", 45000), Row(\"Monitor\", 12000), Row(\"Keyboard\", 2600), Row(\"Mouse\", 1200)]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=[\"item\", \"price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    item|price|\n",
      "+--------+-----+\n",
      "|  Laptop|45000|\n",
      "| Monitor|12000|\n",
      "|Keyboard| 2600|\n",
      "|   Mouse| 1200|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Order = Row(\"item\", \"price\")\n",
    "\n",
    "#order = Order(\"Laptop\", 45000)\n",
    "\n",
    "data = [Order(\"Laptop\", 45000), Order(\"Monitor\", 12000), Order(\"Keyboard\", 2600), Order(\"Mouse\", 1200)]\n",
    "\n",
    "df = spark.createDataFrame(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    item|price|\n",
      "+--------+-----+\n",
      "|  Laptop|45000|\n",
      "| Monitor|12000|\n",
      "|Keyboard| 2600|\n",
      "|   Mouse| 1200|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "col = lit(\"Dummy\")\n",
    "type(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"Laptop\", 45000), (\"Monitor\", 12000), (\"Keyboard\", 2600), (\"Mouse\", 1200)]\n",
    "schema = [\"item\", \"price\"]\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    item|price|\n",
      "+--------+-----+\n",
      "|  Laptop|45000|\n",
      "| Monitor|12000|\n",
      "|Keyboard| 2600|\n",
      "|   Mouse| 1200|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-------+\n",
      "|    item|price|country|\n",
      "+--------+-----+-------+\n",
      "|  Laptop|45000|  India|\n",
      "| Monitor|12000|  India|\n",
      "|Keyboard| 2600|  India|\n",
      "|   Mouse| 1200|  India|\n",
      "+--------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"country\", lit(\"India\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    item|\n",
      "+--------+\n",
      "|  Laptop|\n",
      "| Monitor|\n",
      "|Keyboard|\n",
      "|   Mouse|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.item).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    item|\n",
      "+--------+\n",
      "|  Laptop|\n",
      "| Monitor|\n",
      "|Keyboard|\n",
      "|   Mouse|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df[\"item\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    item|\n",
      "+--------+\n",
      "|  Laptop|\n",
      "| Monitor|\n",
      "|Keyboard|\n",
      "|   Mouse|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.select(col(\"item\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|    item|country|\n",
      "+--------+-------+\n",
      "|  Laptop|  India|\n",
      "| Monitor|  India|\n",
      "|Keyboard|  India|\n",
      "|   Mouse|  India|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.item, df.country).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
